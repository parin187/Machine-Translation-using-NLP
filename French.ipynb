{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "French.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGRDRI2yInu1"
      },
      "source": [
        "import collections\n",
        "import helper\n",
        "import numpy as np\n",
        "import keras\n",
        "import project_test as tests\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ-cR0cBKPAn"
      },
      "source": [
        "import os\n",
        "def load_data(path):\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlNOY7g8Irnu"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAk3tLhWc8pu"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRHRKHUdInvI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cab6c10b-a793-4d09-c8a0-5d224d2b17f1"
      },
      "source": [
        "english_sentences = load_data('small_vocab_en.txt')\n",
        "french_sentences = load_data('small_vocab_fr.txt')\n",
        "print('Dataset Loaded')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikaoM4eVdEHV"
      },
      "source": [
        "## Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALBEsa6edLnQ"
      },
      "source": [
        "Each line in small_vocab_en contains an English sentence with the respective translation in each line of small_vocab_fr. View the first two lines from each file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNfzd4zmInvV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "da350c0d-84a8-458e-d583-b10eee6a5cca"
      },
      "source": [
        "for sample_i in range(3):\n",
        "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
        "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
            "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
            "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
            "small_vocab_en Line 3:  california is usually quiet during march , and it is usually hot in june .\n",
            "small_vocab_fr Line 3:  california est généralement calme en mars , et il est généralement chaud en juin .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnNlnUXKdU3L"
      },
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIzcp3IRInvm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "f52f47d7-9962-4ded-b9d7-41c80b33dff4"
      },
      "source": [
        "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
        "print('{} unique English words.'.format(len(english_words_counter)))\n",
        "print('10 Most common words in the English dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(15)))[0]) + '\"')\n",
        "print()\n",
        "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
        "print('{} unique French words.'.format(len(french_words_counter)))\n",
        "print('10 Most common words in the French dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(15)))[0]) + '\"')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1823250 English words.\n",
            "227 unique English words.\n",
            "10 Most common words in the English dataset:\n",
            "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\" \"usually\" \"never\" \"least\" \"favorite\" \"fruit\"\n",
            "\n",
            "1961295 French words.\n",
            "355 unique French words.\n",
            "10 Most common words in the French dataset:\n",
            "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\" \"jamais\" \"le\" \"l'\" \"généralement\" \"moins\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0LEs39Xdadq"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3qvSBvodc2P"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlofB2hcdfgG"
      },
      "source": [
        "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"cat\" is a sequence of ASCII character encodings. Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
        "\n",
        "We can turn each character into a number or each word into a number. These are called character and word ids, respectively. Character ids are used for character level models that generate text predictions for each character. A word level model uses word ids that generate text predictions for each word. Word level models tend to learn better, since they are lower in complexity, so they will be used in this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35JSnp4jInvw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "9deb70f7-2f5c-4e7f-cbbc-30f57b6242f7"
      },
      "source": [
        "#Tokenize\n",
        "\n",
        "def tokenize(x):\n",
        "    x_tk = Tokenizer(char_level = False)\n",
        "    x_tk.fit_on_texts(x)\n",
        "    return x_tk.texts_to_sequences(x), x_tk\n",
        "text_sentences = [\n",
        "    'The quick brown fox jumps over the lazy dog .',\n",
        "    'By Jove , my quick study of lexicography won a prize .',\n",
        "    'This is a short sentence .']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
            "\n",
            "Sequence 1 in x\n",
            "  Input:  The quick brown fox jumps over the lazy dog .\n",
            "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
            "Sequence 2 in x\n",
            "  Input:  By Jove , my quick study of lexicography won a prize .\n",
            "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
            "Sequence 3 in x\n",
            "  Input:  This is a short sentence .\n",
            "  Output: [18, 19, 3, 20, 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sax-Ko1djXE"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjDMAK3WdmyI"
      },
      "source": [
        "When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0Ncfj2MInv6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "ac5c13da-b94d-4364-876c-54ade70050dc"
      },
      "source": [
        "#padding\n",
        "\n",
        "def pad(x, length=None):\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
        "tests.test_pad(pad)\n",
        "# Pad Tokenized output\n",
        "test_pad = pad(text_tokenized)\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence 1 in x\n",
            "  Input:  [1 2 4 5 6 7 1 8 9]\n",
            "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
            "Sequence 2 in x\n",
            "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
            "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
            "Sequence 3 in x\n",
            "  Input:  [18 19  3 20 21]\n",
            "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjOcUbP9drZU"
      },
      "source": [
        "## Pre-processing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhppaoxoInwE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "0fed1918-5285-4ae1-a36a-b816763c72d7"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "    \n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "# Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(english_sentences, french_sentences)\n",
        "    \n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Preprocessed\n",
            "Max English sentence length: 15\n",
            "Max French sentence length: 21\n",
            "English vocabulary size: 199\n",
            "French vocabulary size: 344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXJlDqc3dxmI"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVWEZhhDdxVo"
      },
      "source": [
        "## Conversion of IDs back to text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOdujgz5dxED"
      },
      "source": [
        "The neural network will be translating the input to words ids, which isn't the final form required. We want the French translation. The function logits_to_text will bridge the gab between the logits from the neural network to the French translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDL4BbtEInwO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27ea6b41-cc78-4e22-808c-5c4e0d25f6db"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "print('`logits_to_text` function loaded.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`logits_to_text` function loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO0YxXXSeAor"
      },
      "source": [
        "## Final Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPECYaO4eARO"
      },
      "source": [
        "This model incorporates both Embedding and Bi-directional RNNs into one model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGB82Uv-InwZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12c3457d-313d-4468-9dbd-c45a324df3be"
      },
      "source": [
        "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "  \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n",
        "    model.add(Bidirectional(GRU(256,return_sequences=False)))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    model.compile(loss = sparse_categorical_crossentropy, \n",
        "                 optimizer = Adam(learning_rate), \n",
        "                 metrics = ['accuracy'])\n",
        "    \n",
        "    return model\n",
        "print('Final Model Loaded')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Model Loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-UgZ30deGs4"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5s-ObP1Inwh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "outputId": "e36e34bb-cd23-42cd-be3f-238ab073e096"
      },
      "source": [
        "def final_predictions(x, y, x_tk, y_tk):\n",
        "    tmp_X = pad(preproc_english_sentences)\n",
        "    model = model_final(tmp_X.shape,\n",
        "                        preproc_french_sentences.shape[1],\n",
        "                        len(english_tokenizer.word_index)+1,\n",
        "                        len(french_tokenizer.word_index)+1)\n",
        "    \n",
        "    history = model.fit(tmp_X, preproc_french_sentences, batch_size = 1024, epochs = 17, validation_split = 0.2)\n",
        " \n",
        "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
        "    y_id_to_word[0] = '<PAD>'\n",
        "    sentence = 'he saw a old yellow truck'\n",
        "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
        "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
        "    sentences = np.array([sentence[0], x[0]])\n",
        "    predictions = model.predict(sentences, len(sentences))\n",
        "    print('Sample 1:')\n",
        "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
        "    print('Il a vu un vieux camion jaune')\n",
        "    print('Sample 2:')\n",
        "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
        "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
        "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/17\n",
            "110288/110288 [==============================] - 47s 422us/step - loss: 2.2710 - accuracy: 0.4973 - val_loss: 1.4315 - val_accuracy: 0.6264\n",
            "Epoch 2/17\n",
            "110288/110288 [==============================] - 45s 406us/step - loss: 1.1413 - accuracy: 0.6872 - val_loss: 0.9174 - val_accuracy: 0.7388\n",
            "Epoch 3/17\n",
            "110288/110288 [==============================] - 44s 403us/step - loss: 0.7660 - accuracy: 0.7761 - val_loss: 0.6173 - val_accuracy: 0.8169\n",
            "Epoch 4/17\n",
            "110288/110288 [==============================] - 45s 404us/step - loss: 0.5011 - accuracy: 0.8507 - val_loss: 0.3731 - val_accuracy: 0.8909\n",
            "Epoch 5/17\n",
            "110288/110288 [==============================] - 45s 404us/step - loss: 0.3015 - accuracy: 0.9118 - val_loss: 0.2521 - val_accuracy: 0.9275\n",
            "Epoch 6/17\n",
            "110288/110288 [==============================] - 45s 404us/step - loss: 0.1982 - accuracy: 0.9429 - val_loss: 0.1707 - val_accuracy: 0.9504\n",
            "Epoch 7/17\n",
            "110288/110288 [==============================] - 44s 403us/step - loss: 0.1485 - accuracy: 0.9570 - val_loss: 0.1424 - val_accuracy: 0.9588\n",
            "Epoch 8/17\n",
            "110288/110288 [==============================] - 44s 401us/step - loss: 0.1134 - accuracy: 0.9672 - val_loss: 0.1323 - val_accuracy: 0.9618\n",
            "Epoch 9/17\n",
            "110288/110288 [==============================] - 44s 399us/step - loss: 0.1021 - accuracy: 0.9703 - val_loss: 0.1133 - val_accuracy: 0.9677\n",
            "Epoch 10/17\n",
            "110288/110288 [==============================] - 44s 398us/step - loss: 0.0950 - accuracy: 0.9722 - val_loss: 0.1086 - val_accuracy: 0.9691\n",
            "Epoch 11/17\n",
            "110288/110288 [==============================] - 44s 396us/step - loss: 0.0776 - accuracy: 0.9773 - val_loss: 0.0956 - val_accuracy: 0.9724\n",
            "Epoch 12/17\n",
            "110288/110288 [==============================] - 44s 402us/step - loss: 0.0682 - accuracy: 0.9799 - val_loss: 0.0883 - val_accuracy: 0.9748\n",
            "Epoch 13/17\n",
            "110288/110288 [==============================] - 45s 404us/step - loss: 0.0592 - accuracy: 0.9827 - val_loss: 0.0784 - val_accuracy: 0.9777\n",
            "Epoch 14/17\n",
            "110288/110288 [==============================] - 44s 402us/step - loss: 0.0534 - accuracy: 0.9845 - val_loss: 0.0956 - val_accuracy: 0.9735\n",
            "Epoch 15/17\n",
            "110288/110288 [==============================] - 44s 400us/step - loss: 0.1114 - accuracy: 0.9685 - val_loss: 0.0944 - val_accuracy: 0.9736\n",
            "Epoch 16/17\n",
            "110288/110288 [==============================] - 44s 401us/step - loss: 0.0629 - accuracy: 0.9817 - val_loss: 0.0881 - val_accuracy: 0.9752\n",
            "Epoch 17/17\n",
            "110288/110288 [==============================] - 44s 402us/step - loss: 0.0465 - accuracy: 0.9864 - val_loss: 0.0721 - val_accuracy: 0.9803\n",
            "Sample 1:\n",
            "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "Il a vu un vieux camion jaune\n",
            "Sample 2:\n",
            "new jersey est parfois calme pendant l'automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8xL8KO6fyn7"
      },
      "source": [
        "The Model achieves an accuracy of 98.03% after validation on 27,753 samples"
      ]
    }
  ]
}